---
layout:     post                    # 使用的布局（不需要改）
title:      MLDS18-RL-Q Learning  # 标题 
subtitle:   Lecture-02              #副标题
date:       2019-07-01              # 时间
author:     ChillyRain      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - RL
---

## Q-Learning: State Value Function

Q-Learning的方法并不直接学习得到policy，它只是一个critic，给定一个policy的情况下，它可以评测出这个policy的好坏。其中一个测评函数为state value
function，记作$V^{\pi}(s)$，表示在使用$\pi$作为policy的情况下，在访问了状态s后未来所能获得的期望收益值。通常使用一个模型，如神经网络来对这个函数建模。

训练得到state value function的方法有两类：Monte-Carlo Search，Temporal Difference

### Monto-Carlo Search

先使用这个policy跟环境互动获得若干个episodes，对每个出现的状态$s$，统计在其出现后直到当前episode结束，所获得的reward之和作为一个估计值$G_s$，在所有episodes中收集到的$(s, G_s)$作为训练样本，使用模型进行拟合。

### Temporal Difference

前一种方法需要整个episode才能得到$G_s$，而这种方法只需要四元组$(s_t, a_t, r_t, s_{t+1})$，并通过以下计算式建立关联

$$
V^{\pi}(s_t) = r_t + V^{\pi}(s_{t+1})
$$

这两种方法各有优劣，前者计算的$G_s$偏倚小，但其相当于多个变量值$r_t$的累加，方差会比较大，而后者只考虑一个$r_t$，所以方差较小，但由于拟合的目标值为$r_t + V^{\pi}(s_{t+1})$，其中第二项本身可能就不准，所以偏倚会相对较大。综上，当样本量足够大时使用前者，否则使用后者。

![MCTD](/img/post-RL-Q-MCTD.png){:height="75%" width="75%"}


## Q-Learning: State-Action Value Function

这个函数记作$Q^{\pi}(s, a)$，表示在访问了状态s且执行动作a后，余下的动作由给定的policy$\pi$来执行至episode结束，所获得的reward期望值。这个函数与State Value Function存在如下关系：

$$V^{\pi}(s) = Q^{\pi}(s, \pi(s))$$

训练的方法同样也是TD或者MC，计算这个函数的另一个好处是，可以由这个函数induce出一个policy，大体流程如下：对于当前policy $\pi$，先计算其对应的的$Q^{\pi}(s, a)$，再依据下式得到新的policy $\pi'$：

$$\pi'(s) = argmax_a Q^{\pi}(s, a)$$

![Proc](/img/post-RL-Q-induce.png){:height="75%" width="75%"}

可以证明：这个新的$\pi'$要比原来的$\pi$更"好"。这里更"好"表示对于所有可能的状态s，都有$V^{\pi'}(s) >= V^{\pi}(s)$。下面是具体的证明过程，每当使用$\pi'$的动作来代替$\pi$时，都能使state value function进一步提升(due to argmax)。

![Prof](/img/post-RL-Q-prof.png){:height="75%" width="75%"}

在以上框架之下，还需要几个改进点才能让训练得以顺利进行
1. 当使用TD来训练时，由于等式右侧的目标中包括$Q^{\pi}(s, a)$，即拟合目标是变动的，这会使训练较为困难，一个改进的方法是固定右侧这个值，在参数更新了N次后才更新一次这个目标网络
2. Exploration: 拟合的函数是$Q^{\pi}(s, a)$，相当于希望准确得到每个$(s, a)$所对应的期望收益；但是如果$\pi$在遇到s时都只是固定输出一个a，那么很多$(s, a'), a \ne a'$都没办法准确计算得出，那么在使用这个Q来Induce下一轮的策略时也不一定就是最优的。简单的改进方法是增加一些随机性，收集到更多样的样本，即在不同的$(s, a)$下获得了多少reward。例如：
![Explore](/img/post-RL-Q-explore.png){:height="75%" width="75%"}
3. Replay Buffer: 将训练中所有策略所收集到的episode都放到一个Replay Buffer之中，每次训练时从中随机采样一部分出来估计Q-Function。如果是使用TD来作估计方法的话，这种方式是合理的，因为TD只使用四元组$(s_t, a_t, r_t, s_{t+1})$，在给定$s_t$的情况下，与policy直接相关的只有$a_t$，而$r_t, s_{t+1}$都是由环境给出的，与policy无关。而想要得到Q函数对policy的依赖是从第二步开始，这第一步的action是Q函数的参数，所以从其它策略收集到的四元组也是可以复用的。

把所有上面这些东西放在一起，就有了下面的Q-Learning的算法流程。

![Algo](/img/post-RL-Q-algo.png){:height="75%" width="75%"}

这里有一点需要说明，就是使用TD做拟合时，目标值$y = r_t + max_a \hat{Q}(s_{i+1}, a)$，其中的第二项可以使用$\hat{Q}(s_{i+1}, \hat{\pi}(s_{i+1}))$，这时策略的使用更一致，$Q$和$\pi$使用的均为"上次"策略。而上面所使用的max实际上是在这第一步时使用了"当前"策略来决定的action，而不是"上次"策略，因为"当前"策略就是用$argmax_a Q(s,
a)$来决定的。如上面改进点1所述，理论上就是应该使用当前策略的，但是为了训练稳定，才选择使用了上次策略，而在这第一步上，能用上当前策略就没有不使用的理由。

## Improvement: Double DQN

下面是几种对原始Q-Learning的改进，第一种是Double DQN，这种改进的动机就是上节最后提到的目标值

![Double](/img/post-RL-Q-double.png){:height="75%" width="75%"}

这里的max可能会使估计的目标值偏高。原因在于在当前轮的迭代中，如果右侧的目标值由于参数误差等原因而过大，那么左侧所拟合的函数就会偏大，而在下一次迭代中，这个左侧的函数就会出现在右侧，作为下一轮迭代的目标值，这种误差会被进一步放大，迭代次数越多放大的程度越大，最终造成overestimate。

这里的解决方案就是对于目标值的估值，使用两个网络，一个用于选择action，另一个用于计算Q函数值，减小出现过高估计的可能性。

## Improvement: Dueling DQN

Dueling DQN的本质是认为直接Q函数的形式没有任何约束，过于灵活，容易出现过拟合。因此在Q函数的形式上加一个先验结构

$$Q(s, a) = V(s) + A(s, a)$$

第一项用来评估当前状态的好坏，与具体的action无关，而第二项则是约束同一个状态下所有action的取值之和为零，表示在此状态下各个action的优劣。

![Duel](/img/post-RL-Q-duel.png){:height="75%" width="75%"}

## Improvement: Prioritized Replay

这个的思路跟boosting有点像，每次从Replay Buffer中采样时，以更大的概率选择上轮迭代时TD Error较大的样本。

## Improvement: Multi-Step

这个的意思是采样时不像MC一样要存整个episode，也不像TD一样只存四元组，而是存N步的结果，这样就在MC和TD之间取了一个折衷，合并两者的优势，既不会让方差太大，也不会让偏倚过大。此时的训练目标值为

![MStep](/img/post-RL-Q-mstep.png){:height="75%" width="75%"}

## Improvement: Noisy Net

这个是Exploration的另一种方案，与Epsilon Greedy不同，它在一个episode开始前先在Q函数的参数中引入随机噪声，这样它在整个episode期间的策略是不变的。而之前的方案是在每一步都有随机性，同一个episode遇到同一个State时采取的action都有可能是不同的。

![Noisy](/img/post-RL-Q-noisynet.png){:height="75%" width="75%"}

## Improvement: Distributional Q-Function

Q函数只是计算给定s和a的情况下，未来所获得的reward的期望值，而不同的分布可能会得到相同的期望值。所以可以做得更细致一些，每个Q输出不只是一个值，而是一个概率分布。

![Dist](/img/post-RL-Q-distQ.png){:height="75%" width="75%"}

## Q-Learning for Continous Action

对于连续变量的action，主要难点在于argmax不容易算，以下是几点解决方案

1. 采样N个action，代入Q函数，取取值最大的那个action
2. 使用梯度下降来找使Q最大的action
3. 通过设计网络结构使得argmax容易计算
![Cont](/img/post-RL-Q-cont.png){:height="75%" width="75%"}
4. 使用Actor-Critic，而不是DQN




