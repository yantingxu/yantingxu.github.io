---
layout:     post                    # 使用的布局（不需要改）
title:      MLDS18-GAN-Tips for Improving GAN  # 标题 
subtitle:   Lecture-05              #副标题
date:       2019-06-04              # 时间
author:     ChillyRain      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - GAN
---

## Problem of GAN

原始的GAN理论上是可行的，但实际上会遇到训练不动的问题。主要原因在于，在达到最优解之前，$P_G$和$P_{data}$之间几乎是毫不相同的，可能的原因有2个：

![overlapped](/img/post-GAN-tips-overlapped.png){:height="75%" width="75%"}

当两个分布完全不同时，它们的JSD总是通达到最大值$-2log2$(D总是能达到100%的准确率)。由于Loss总是一样的，而学习算法SGD又是依赖梯度局部信息，这就造成G的学习几乎毫无动力，因为无法往哪个方向移动，Loss都是不变的。

解决方案就是不再使用JSD，而是使用一种能在without overlapped的情况下也能反应分布间距离的测度。

## Least Square GAN

一种简单的解决方法就是把D从二分类器换成一个回归器，不再使用sigmoid约束输出的值要在0~1之间，而是可以是任意值，希望它能在两个分布之间毫无overlapped的情况下，也能表达某种意义下两个分布之间的距离。

![LSGAN](/img/post-GAN-tips-LSGAN.png){:height="75%" width="75%"}

## Wasserstein GAN (WGAN)

WGAN是另一种解决方案，它使用Earth Mover's Distance来代替JSD。这个测度的意义是把两个分布看作两堆土，每个位置上土的量就是其对应的概率，把一堆土通过移动变成另一堆土所需要最小工作量(土量*距离)就是一定程度上表达两个概率分布的距离，而且即使在毫无overlapped的情况下也适用。

![DIS](/img/post-GAN-tips-WDis.png){:height="75%" width="75%"}

与JSD的直观对比如下，JSD在毫无overlapped的情况下无法为G提供有效的指导，而Earth Mover's Distance却可以做到。

![CMP](/img/post-GAN-tips-compare.png){:height="75%" width="75%"}

剩下的就是如何计算这个测度，以及如何把这个测度融入GAN的框架。从原始定义出发，可以推导出以下的$V(G, D)$来表示Earth Mover's Distance，推导过程请见(Martin Arjovsky, et al., arXiv, 2017)


$$V(G, D) = \max_{D \in 1-Lipschitz} {E_{x \sim P_{data}} D(x) - E_{x \sim P_G} D(x)} $$

直观上就是让真实样本在D上的打分高，而在生成样本上打分低。这里对D有约束，它只能在1-Lipschitz的函数集中来取，它的定义是

$$||f(x_1) - f(x_2)|| <= ||x_1 - x_2||$$

等价于

$$\nabla f(x) <= 1, \forall x$$

也就是说要求函数$f$要足够平滑，不能在其域空间内的值变化过大或过小。如果没有这个约束条件，那么D总是倾向于会给真实样本$\infty$而给生成样本$-\infty$，造成最优化问题无法收敛。实际计算中对1-Lipschitz的处理有几种方案：

* Weight Clipping (Martin Arjovsky, et al., arXiv, 2017): 简单来说，就是每次更新参数后，看它是否在$-c$到$c$之间，如果不是的话就做剪裁；由于参数作用被约束在一个范围内，那么就保证它不会引起局部的突增或突减。比较直觉的方法。

* Spectrum Norm (Miyato, et al., ICLR, 2018): 保证每个梯度值的norm小于1，略。

* WGAN-GP: 将约束转化为Loss函数的一部分，这个也是比较常用的方法，比如SVM也用过类似的方法。

## WGAN-GP

在原始的WGAN的Loss中新增项，保证任意梯度值都不超过1，如果超过就会引入额外的Loss。但是这个"任意梯度"在实现中是没办法做的，只能采样一些点出来，而要采样哪些点才是合理的呢？由于每次迭代中，D都的作用是用于衡量$P_{data}$和$P_G$之间的距离，同时希望后者是快速接近前者，因此从两个分布及二者的路径中采样看起来是合理的。

![GP1](/img/post-GAN-tips-GP1.png){:height="75%" width="75%"}

![GP2](/img/post-GAN-tips-GP2.png){:height="75%" width="75%"}

进一步地，从实验上观测到，当D达到最优值时，几乎在每个位置上的梯度都达到了1，这是可理解的，因为1-Lipschitz约束跟后面最优化的Distance本身就是互相约束的，后者希望这个值越大越好，甚到达到无穷大，而前者就是想让$f$是平滑的，使这个值在一个可控的范围内，因此这个约束最优化的结果就是让前者达到这个约束的边界值，即$\nabla f(x) = 1$。基于这种现象，可以进一步修改，把max操作符去掉：

![GP3](/img/post-GAN-tips-GP3.png){:height="75%" width="75%"}

最后给出WGAN的整体算法流程

![algo](/img/post-GAN-tips-algo.png){:height="75%" width="75%"}

## Energy-based GAN 

TODO

## Loss-Sensitive GAN

TODO



