---
layout:     post                    # 使用的布局（不需要改）
title:      MLDS18-RL-Sparse Reward  # 标题 
subtitle:   Lecture-04              #副标题
date:       2019-07-02              # 时间
author:     ChillyRain      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - RL
---

Sparse Reward是指在一个复杂任务中，初始的Actor比弱，以至于与环境互动时极少能得到正向反馈，actor在不知道怎样做才是正确的，导致学习困难的问题。

## Reward Shaping

除了环境给出的reward之外，再人工加一些额外的reward以帮助actor学习。比如机器人在做一个复杂动作任务时，只有在完成后环境才给出正向reward，可以添加一些反应进展的人工reward，让机器人能更快地学会要朝哪个方向去做。

添加好奇心的人工reward是其中的一种应用，如上所述，在计算每个Action所带来的收益时额外把好奇值加上。

![ICM01](/img/post-RL-Sparse-ICM01.png){:height="75%" width="75%"}

ICM Module内部结构如下图所示，其中有2个关键点，第一是输出的$r_t$表示在给定了$s_t, a_t$的情况下，当前模型Network 1对$s_{t+1}$的可预测性，越不容易预测到则reward越大，反映出对actor冒险action的鼓励；第二是对于状态要提取出与action相关的特征，一些不依赖于Action的无用特征需要过滤掉，比如游戏画面中的背景的风吹草动，这就是Network 2所做的事情。

![ICM02](/img/post-RL-Sparse-ICM02.png){:height="75%" width="75%"}


## Curriculum Learning

这种方法是为actor定制一系列从易到难的任务，循序渐进地学习，最终完成任务。

一种方案是使用Reverse Curriculum
Generation，它的主要思路是从目标状态$s_g$进行反推，先从$s_g$周围采样一些状态出来$s_1$，让actor学习如何从它们出发达到目标状态（这是一个相对简单的任务），并统计出每个起始状态到目标状态所获得的$R(s_1)$。如果reward过大或过小，则说明从这个状态出发过于简单或者过于复杂，不符合当前阶段的学习难度，就把这些状态过滤掉。在余下的每个状态周围再采第二轮样本$s_2$，作为下一个阶段学习的起始状态，让actor学习如何从它们开始达到$s_g$，这就是一个相对于第一轮更困难一些的任务了。按照以上流程反复迭代，从易到难，最终能让actor完成困难任务。


## Hierarchical Reinforcement Learning

这种方法是将一个任务划分为多级任务，比如写一本书是一个困难任务，但是可以分解为写一个章节，写每一段，写每一句，逐层实现。每个任务都对应一个actor。high-level actor负责在整上规划，并为low-level actor提出子目标，而low-level actor的任务负责局部的实现。当low-level actor没能实现子目标时，说明子目标提的有问题，所以high-level actor也会受到惩罚。


