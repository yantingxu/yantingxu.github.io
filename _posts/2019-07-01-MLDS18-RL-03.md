---
layout:     post                    # 使用的布局（不需要改）
title:      MLDS18-RL-Actor Critic  # 标题 
subtitle:   Lecture-03              #副标题
date:       2019-07-02              # 时间
author:     ChillyRain      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - RL
---

Actor-Critic方法是整合了Q-Learning和Policy Gradient两种方法，期望二者能优势互补。

## Asyncchronous Advantage Actor-Critic (A3C)

这个方法可以看作是使用Q-Learning来改进Policy Gradient的方法。之前在给每个action计算reward时，使用的是带baseline的未来打折收益的期望值，如下图所示。由于期望值没法直接计算，而是只能从样本中累加得到，由于涉及多变量累加，就可能造成方差较大，以致于训练目标不稳定。

![PG](/img/post-RL-AC-PG.png){:height="75%" width="75%"}

解决问题的一个方案是先使用模型对训练目标做拟合，在训练PG时以模型的输出作为reward。从概念上，我们希望这个reward表示在状态$s_t$下执行$a_t$所能在今后带来的收益，很自然地跟Q-Learning中的$Q(s, a)$所表达的意义一致；同理，baseline也可以由$V(s)$来表示。

![PG](/img/post-RL-AC-reward.png){:height="75%" width="75%"}

同时，由于Q函数和V函数存在以下关系，所以只用模型拟合一个V函数就足够了。

$$Q(s_t, a_t) = r_t + V(s_{t+1})$$

于是就得到了A2C的算法流程图：

![A2C](/img/post-RL-AC-A2C.png){:height="75%" width="75%"}

然后还有两个Tips
1. policy对应的模型$p_{\theta}(a\|s)$和V函数对应的模型$V(s)$可以都是神经网络，而且输入都是state，这样就有可能让二者在开始的几层做参数共享，仅在后边几层各做各的
2. 可以对$p_{\theta}(a\|s)$加一个基于最大熵的regularization，避免过拟合（概率分布平滑）

到目前为止都是A2C，最后一个A是Async，是一个工程上使用多机训练的方式，Parameter Server在收到任意worker返回的梯度后就立即更新参数，即使其它worker的参数可能已经过时。其优点在于能快速频繁更新参数。

![Async](/img/post-RL-AC-async.png){:height="75%" width="75%"}


## Pathwise Derivative Policy Gradient

这个方法与上一个相反，可以看作是使用PG来改进Q-Learning的方法。如Q-Learning那一节所述，它是不容易处理continuous action，而这个方案就能解决这个问题。

具体来说，就是不再需要使用argmax从Q函数里induce一个policy出来，而是直接构建一个policy network来执行这个argmax的动作，接收一个state输出在当前Q条件下的最优action。其实它跟GAN很类似，policy network充当generator的角色，而Q network充当discriminator的角色。训练的流程跟GAN也很类似，如下：

![GAN](/img/post-RL-AC-GAN.png){:height="75%" width="75%"}

具体的算法流程：

![Algo](/img/post-RL-AC-algo.png){:height="75%" width="75%"}



