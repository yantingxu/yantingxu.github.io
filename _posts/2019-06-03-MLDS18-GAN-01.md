---
layout:     post                    # 使用的布局（不需要改）
title:      MLDS18-GAN-Introduction # 标题 
subtitle:   Lecture-01 #副标题
date:       2019-06-03              # 时间
author:     ChillyRain      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - GAN
---

## Basic Idea of GAN

GAN为Generative Adversarial Network的简称，是一种生成模型，目标在于学习得到据的概率分布$P(x)$。与传统的生成模型不同，GAN并不直接对概率分布建模，而是通过指定一个先验分布（随意，通常是正态分布或者均匀分布），再配合一个学习得到的生成器（Generator）来将先验分布进行转换生成目标分布$P(x)$。生成器通常是一个神经网络，而神经网络的复杂性使得它可以模拟任何转换函数（universal
approximator），因此理论上它可以输出任意概率分布。这种模型优势在于采样比较容易（在先验分布上采样，输入到生成器中即可得到样本），但是相对地，在给定一个样本的情况下，想得到对应的概率就比较困难了。

学习任何模型都需要外界的驱动信号，比如监督学习中的各种loss，这个Generator也不例外，它的驱动信号来自于Discriminator，即GAN的另一个组成部分。举例来说，我们想得到一个图像生成器，那么我们就希望Generator的输出一幅高质量的图片。如果生成器中的参数不够好，那么生成的图片很可能是模糊的，甚到只是一堆噪声，此时它就需要一个信号来告诉它如何改进参数。在这个例子中，图片的质量并不容易通过设计一个loss来实现（因为图像的像素间存在strcture，而structure error很难由一个简单的loss函数捕获，类似的还有NLP中的POS序列等），此时可以训练一个Discriminator作为真伪图片鉴别器，当真图片作为输入时它输出为1，当以生成的图片作为输入时，它的输出为0，并将这个loss反馈给Generator来进行参数调整。Generator和Discrimiator交替进行训练，二者的能力都不断提升，最终期望Generator可以生成无法与真实图片区分的高质量图片。

![Alternative](/img/post-GAN-evolve.png){:height="75%" width="75%"}

正如前面所述，GAN由Generator和Discriminator两部分构成，训练时也是需要交替训练，每次迭代中，前者为后者提供负样本，而后者为前者提供驱动信号。

![Algorithm](/img/post-GAN-basic-algo.png){:height="75%" width="75%"}


## GAN as structured learning

structure
learning通常的模型输出都不是单一个类别标签或者数值，而是由具有依赖关系的多个组件而构成的一个整体，比如机器翻译中输出的词序列，依存解析中的语法树，以及上面例子提供的图片(由具有相关性的像素组成)。由于输出的组件之间具有结构信息，这也要求模型在学习中能够捕获这类信息，以在后续的应用中提供关键的泛化能力。如果将整体输出看作一个标签的话，那么标签的数量是巨大的，每个标签对应的训练样本几乎为零，如果模型不能通过结构化信息进行泛化的话，那么模型可用性几乎不存在。

因此在面对此类问题时，模型需要have a big picture in mind（全局信息），并以此为指导来逐个输出每个component（局部信息）。GAN可以看作strcutre learning，因为Discrimiator可以看作一个是具有全局信息指导者（什么样的图片看起来像是真实的），这类全局信息会通过训练过程而固化到Generator之中，训练完成后，Generator就可以在全局信息的指导下生成每一个component（具有依赖关系的像素）。

![StructureLearning](/img/post-GAN-sl.png){:height="50%" width="50%"}


## Can Generator learn by iteself?

Generator自己可以不依赖Discrimiator来学习吗？可以收集到的真实的图片集作为Y，把从先验分布随机生成的向量集作为X，像监督学习一样去训练。这样看起来可行，但实际上有问题。

X和Y并没有明显的对应关系，因此只能随机匹配生成$(x_i, y_i)$，由于二者之间并没有匹配的理由，这种随机性就造成了学习的不稳定。一种解决方案是在Generator前面再加一个从图片到code的编码器，把Generator看作解码器，构成一个AutoEncoder，以reconstruction error（通常为l2
distance）作为驱动信号进行学习，这样编码器侧就约束code必须与图片相关（与图片集相关的统计规律）。进一步地，AE只是孤立地在code空间和图片空间中进行点与点的映射关系的学习，可以采用VAE做进一步的平滑，使code空间中的点即使不在训练集中，也能一定程度上生成图片。

![VAE](/img/post-GAN-VAE.png){:height="75%" width="75%"}

但是这里还是有问题，无论是AE还是VAE，都是采用了像l2 distance这种测度作为loss，而这种loss是捕获不到结构化信息的，它并不能完全合理地度量图片间的距离，比如下面这个例子。也就是说，Generator在学习时，这种loss无法提供structure error的驱动信号，而这正是Discrimiator的意义所在。

![VAE](/img/post-GAN-structure-err.png){:height="75%" width="75%"}


## Can Discriminator generate?

既然Discriminator可以从全局角度评判生成的样本是不是足够好的，那么它可以自己来生成样本吗？如果通够遍历这个Discriminator的domain，那么就可以选出打分最高的样本作为输出。某些场景下，模型可以通过添加一些结构独立性假设来解决这个问题，比如在序列标注任务中，可以通过马尔科夫假设，让HMM/CRF模型通过动态规划来得到打分最高的输出。但这种假设本身有限制，而且并不是在所有情况下都适用的。

另一个问题是负样本从哪里获得？我们收集的真实图片都是正样本，而负样本在训练过程中也是必需的。可以随机生成一个噪声图片，但是此时任务过于容易，学得的Discriminator也质量不高，从而生成的样本质量也不会太高。此时就需要迭代地进行训练，先学一个弱的模型，由它生成一堆样本作为负样本，回过头来重新训练，反复几次，这样会使模型越来越强，生成的图片也会越来越接近真实图片的质量。但是由于需要迭代，每次迭代都要从模型采样出样本，而如上一段所述，从Discrminator生成样本是比较困难的，迭代会让这个问题更加严重。

如果加入了Generator，那么生成负样本的问题就迎刃而解了。


## GAN = Generator + Discrimiator

![CONS](/img/post-GAN-cons.png){:height="75%" width="75%"}

二者各自有自己的优势，也有各自的问题，GAN整合了二者的优势，让整个训练和推断成为可行。

![GAN](/img/post-GAN-complement.png){:height="75%" width="75%"}




