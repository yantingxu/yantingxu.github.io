---
layout:     post                    # 使用的布局（不需要改）
title:      MLDS18-GAN-Feature Extraction  # 标题 
subtitle:   Lecture-06              #副标题
date:       2019-06-04              # 时间
author:     ChillyRain      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - GAN
---

原始的GAN的输入只是一个从先验分布上采样下来的向量，固然这个向量服从先验分布，但是D所提供的学习目标仅仅是输出的图片要真实，而这个向量本身各维度有什么意义是没有约束的，即使每个维度是各种实际物理维度的耦合，也不会影响学习目标。这节课讲的就是如何添加一些约束，让这个向量是有意义的，以致于可以在一定程度上控制GAN的输出，甚至将其使用于其它任务。

## InfoGAN

InfoGAN的约束的手段是让每个维度的值与生成的图片之间的互信息最大化（非偶然共现），也就是说，让每个维度能编码图片中的一些信息。

$$I(x, c) = H(c) - H(c|x)$$

其中$x$表示生成的图片，$c$表示latent code，即先验向量的某一维。由于$c$服从先验分布，所在等式右侧第一项可以忽略，如果要最大化$I(x,
c)$，就相当于最小化$H(c\|x)$，即给定一张图片的情况下，$c$的取值总是较为集中的。当然，在无约束条件下，为每个$x$单独输出一个有区分度的$c$是最优的（最佳区分度，表征具体图片的完整详细的信息），但是由于$c$是有先验分布的，所以二者只能相互约束，让$c$对于$x$有一定的区分度，但又不能偏离先验分布太远，也就是要求$c$在有区分度的同时还要有一定的泛化能力，表达某一类图片的共有特征。例如，人脸图片中的性别，是否带眼镜，手写数字图片中的数字，倾斜度等。

[原始论文](https://arxiv.org/abs/1606.03657)中经过一系统的推导和简化，得到下面这种网络结构，即额外使用一个分类器来表达互信息的约束，要求从图片中可以反过来infer出原始的$c$。另外，$z'$表达的是无法由$c$表达出的其它信息。

![InfoGAN](/img/post-GAN-feature-infoGAN.png){:height="75%" width="75%"}

## VAE-GAN

VAE-GAN可以看作是VAE和GAN的互相补充，通过VAE的reconstruction loss约束了$c$必须要包含真实图片集中具有统计意义的信息。之所以说这是两个模型的相互补充，一方面，VAE的预训练让GAN-Generator初期就有了能生成图片的能力，而不至于在随机初始化参数后抓瞎；另一方面，VAE的l2 distance无法捕获图片中的结构化错误，而GAN-Discriminator在这方面提供了帮助。

![VAEGAN](/img/post-GAN-feature-VAE.png){:height="75%" width="75%"}

以下是VAE-GAN的具体算法流程。左下角还给出了另一种D的实现方式，即表达成三分类器，用于判别真实图片/生成图片/重构图片，重点在于能区分出后两者。

![VAEGAN-Algo](/img/post-GAN-feature-VAE-algo.png){:height="75%" width="75%"}

## BiGAN

BiGAN是另一种约束latent code的方式，基本思想也是一样，在先验分布和包含图片信息的详细程度上做折衷。与上面不同的是，它将encoder和decoder分开，学习目标是让二者输入输出的联合概率分布尽量接近，具体结构如下：

![BiGAN](/img/post-GAN-feature-BiGAN.png){:height="75%" width="75%"}

以下是具体的算法流程

![BiGAN-Algo](/img/post-GAN-feature-BiGAN-algo.png){:height="75%" width="75%"}

## Triple GAN

Triple GAN (Chongxuan Li, Kun Xu, Jun Zhu, Bo Zhang, “Triple Generative Adversarial Nets”, arXiv 2017) 的主要目标是要学习得到一个分类器C，本质上是一种半监督学习方法。网络结构图如下：

![TriGAN](/img/post-GAN-feature-triGAN.png){:height="75%" width="75%"}

其中Y是condition，而X是生成的样本。右侧的G和D组成了conditional GAN，而在训练C的时候，不但使用了真实样本集，还使用了G生成的样本集，并利用了D来判别C输出的Y与X是否匹配。

## Feature Disentangle

这个是借鉴了domain-adversarial training的思想，所以先说这个：主要目的是想要学习domain independent的feature，比如颜色背景不同的手写数字图片中提供通用特征，用于数字分类。网络结构如下，在特征提取之后，除了接入原先的分类器，再添加一个用于区分domain的分类器，特征提取器的学习目标就是要最大化前者的分类精度，同时让domain分类器无法区分特征是从哪类图片中提取出来的，从而驱动提取的特征是domain independent。

![Domain](/img/post-GAN-feature-domain.png){:height="75%" width="75%"}

Feature Disentangle跟上面提到的方法不同，它显式地把特征区分开，并希望每个特征所代表的含义都是单一的。以语音特征为例，下图的上半部分是一个seq2seq auto-encoder的变形，主要就是把特征分成了内容相关和语者相关两部分，并希望学习结束时它们真可以的提取出相对应的单纯的特征；下半部分是表明如何通过domain-adversarial
training来训练出所希望的内容相关的特征提取器，这实际也是一个GAN，左侧为Generator而右侧为Discriminator。Discriminator接收两个特征向量，并判断这两个向量是否来自同一个语者，最终希望Generator可以骗过Discriminator从而产出只与内容相关的speaker-independent的特征。

![Entangle](/img/post-GAN-feature-disentangle.png){:height="75%" width="75%"}



