---
layout:     post                    # 使用的布局（不需要改）
title:      MLDS18-RL-Imitation Learning  # 标题 
subtitle:   Lecture-05              #副标题
date:       2019-07-02              # 时间
author:     ChillyRain      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - RL
---

这个比Sparse Reward的条件更严苛，actor无法与环境互动来获得reward反馈（或者根本就无法定义出合适的reward函数），只能看着expert的展示来学习。


## Behavior Cloning

这个其实就是监督学习，学习给定state的情况下应该输出什么action。主要问题在于expert没遇到的state没有对应的数据，也就没办法学习，一旦在应用中出现这种state就完蛋了。更重要的，在RL的场景下，当前的action会影响后面出现的state（不像监督学习一样是iid的），如果不能完全复制expert，那么遇到没见过的state的可能性会很高（Data Mismatch）。

Data
Aggregation可以一定程度上缓解这个问题，主要思路是先学习一个actor出来，让它与环境互动（但是得不到reward），它所遇到的每个state都让人工标记一下应该采取什么样的action。因为学习得到的Actor并不与expert完全一致，所以在与环境互动中能看到一些之前没遇到的state，人工标注后可以继续用来学习。这种方式需要人工标注，费时费力，能多看到一些state，但也保证不了覆盖率。

另外，这种Cloning方法还有一个问题，如果模型本身的容量(capacity)不足以完全复制expert的action，那么它有可能只能学到一部分action，但它又无法区别哪些行为是更重要的，所以最终学成什么样子也不好说。

## Inverse Reinforcement Learning (IRL)

RL的正常流程是给定的reward + env，通过互动获取episodes来训练得到一个最优的actor；IRL是相反的，给定了expert演示的episodes的前提下，先反向学习得到一个reward function，当得到了这个reward function之后，就可以再RL正向学习actor。综上，学习的整体路径是expert episodes => reward function => actor，先将expert的episodes泛化成一个reward
function（而不是直接从episodes学习），再以此来驱动actor的学习。

先学习reward function的动机是，虽然episode中的行为可能很复杂，但是reward function却不一定复杂。比如，人只是为了活着，却搞出这么多事。

整体的结构图如下，其实跟GAN差不多是一个意思，actor充当generator，而reward function则充当discriminator。

![IRL](/img/post-RL-IL-IRL.png){:height="75%" width="75%"}

## 其它

* Third Person Imitation Learning: actor从第三人称视角观察expert的episodes，再进行学习；架构中需要一个Domain Adverserail Training，希望提取到的特征不依赖于第一/第三人称视角(TODO)。
* Sentence Generation: MLE学习就像是Behavior Cloning，而SeqGAN则像是IRL。




