---
layout:     post                    # 使用的布局（不需要改）
title:      MLDS18-RL-Policy Gradient  # 标题 
subtitle:   Lecture-01              #副标题
date:       2019-07-01              # 时间
author:     ChillyRain      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - RL
---

## Policy Gradient

Reinforcement Learning由三个组件组成，Actor + Env + Reward Function，其中后两者是无法控制的，能学习得到的只有Actor的Policy，即在在遇到不同的场景时应该如何采取行动。RL有两类解决方案，Policy Gradient和Q-Learning，这里只介绍前者，而它的思路就是只直接学到一个Policy。Policy可以表示为一个函数，接收一个状态state，输出一个action，或者更一般地，一个关于action的条件概率分布$p_\theta(a \| s)$。

学习之前需要先收集数据，方法就是先使用一个非最优的policy，甚至是随机初始化的policy来跟Env做互动，直到达到终止状态，整个序列称为一个episode，可以表示为$\tau$。每个episode所对应的概率可以如下表示，虽然那些不带$\theta$的概率是没法算出来的

$$
p_{\theta}(\tau) = p(s_1) \Pi_{t} p_{\theta}(a_t|s_t)p(s_{t+1}|s_t, a_t)
$$

除此之外，每个episode都有从Reward Function中得到的奖励值，记作$R(\tau) = \Sigma_t r_t$。通过多次与环境的互动后，就可以得到若干个episode。

![Overall](/img/post-RL-PG-overall.png){:height="75%" width="75%"}

很自然地，我们希望得到的policy可以让期望所获得的reward值最大化，这就需要通过调整参数$\theta$来达到。因此，最优化的目标就可以写成

$$
\bar{R}_\theta = \Sigma_{\tau} R(\tau) p_{\theta}(\tau) = E_{\tau \sim p_{\theta}(\tau)} R(\tau)
$$

一般情况下，关于输入的期望可以近似使用样本平均来替代，比如上面的$\bar{R}_\theta$可以写成

$$
\bar{R}_\theta = \frac{1}{N} \Sigma_{n=1}^N R(\tau_n)
$$

但是这里出现的问题就是目标函数里面没有了待优化参数$\theta$，也就没办法使用梯度来调参了，所以还需要做一些变形。直接看梯度的表达式

$$
\nabla \bar{R}_{\theta} = \Sigma_{\tau} R(\tau) \nabla p_{\theta}(\tau) = \Sigma_{\tau} R(\tau) p_{\theta}(\tau) \frac{\nabla p_{\theta}(\tau)}{p_{\theta}(\tau)} = \Sigma_{\tau} R(\tau) p_{\theta}(\tau) \nabla logp_{\theta}(\tau) = E_{\tau \sim p_{\theta}(\tau)} R(\tau) \nabla logp_{\theta}(\tau)
$$

此时我们再用样本平均代替期望，可以得到

$$
\nabla \bar{R}_\theta = \frac{1}{N} \Sigma_{n=1}^N R(\tau_n) \nabla logp_{\theta}(\tau)
$$

进一步地，参考$p_{\theta}(\tau)$的表达式，代入上式中，与$\theta$无关的项都可以去掉，得到

$$
\nabla \bar{R}_\theta = \frac{1}{N} \Sigma_{n=1}^N \Sigma_{t=1}^T R(\tau_n) \nabla logp_{\theta}(a_t|s_t)
$$

到此为止就是Policy Gradient更新参数时梯度值的计算方法，每次更新后需要与环境重新互动生成新的episode，再进行下一次的参数更新，迭代下去。整体的流程如下：

![Params](/img/post-RL-PG-params.png){:height="75%" width="75%"}

## Improvement Tips

观察上面计算的梯度值的表达式，可以解释为：整个episode中的所有Actions出现的概率$p_{\theta}(a_t\|s_t)$一致地提升或下降，且变化的幅度正比于整个episode所获得的reward，即$R(\tau)$。这里有明显不合理的地方，首先整个episode获得的reward多不代表其中所有的action都是对的，其次reward值的取值如果都是正的，那么代表所有action都要提升，相对地，由于归一化条件，那么没有被采样到的action就要下降。

对于第二个问题，更好的假设是让采样到的action有优有劣，而没有采样到的action则认为不优不劣。可以通过添加一个baseline来达到这样的效果，比如可以计算所有episode的reward的平均值，让高于均值的为优，低于均值的为劣。修改梯度值为

$$
\nabla \bar{R}_\theta = \frac{1}{N} \Sigma_{n=1}^N \Sigma_{t=1}^T [R(\tau_n) - E[R(\tau)]] \nabla logp_{\theta}(a_t|s_t)
$$

对于第一个问题，如果在每一步都能获得一个独立的reward，即$r_t$，而不是只能在最后获得一个整体的$R(\tau)$的话，那么可以计算每一个action所带来的当前收益和未来的收益(discounted)，这个收益值可以表示为

![Tips](/img/post-RL-PG-tips.png){:height="75%" width="75%"}

其中baseline对应地改为与这个state相关的值，而不必与整个episode相关。比如可以计算为面对某个固定的state时所获得收益的均值，这样就可以一定程度上区分样本中采取的action是优是劣。

这里面的收益值及收益值的baseline在后面可以使用Q-Learning中的方法来估计，此时就形成了一种Actor-Critic方法，这个后面再说。

## From On-Policy to Off-Policy

上面的方法是在每次迭代中，使用当前的policy来收集数据，使用数据来更新policy，再进行下一次的数据收集，这种方法称为On-Policy策略。缺点是每次迭代都要使用当前的policy重新收集数据，比较费时。对应的有一种方法为Off-Policy方法，它可以使用非当前policy所收集的数据来更新当前的policy，而这种方法的关键是使用了Importance Sampling。

Importance Sampling要解决的问题是，我们想计算$E_{x \sim p(x)} [f(x)]$，但是我们只有从另一个分布$q(x)$得到的样本，那么如何计算前面这个期望 值。解决方案如下，实质就是根据p和q两个分布来调整每个q样本的权值，再根据q来计算这个加权期望。

![Mean](/img/post-RL-PG-sampling-mean.png){:height="75%" width="75%"}

虽然计算得到的期望值是一样的，但是方差却不一样，且方差依赖于p和q之间有差异有多大。如果二者差异很大的话，那么就有可能有一些样本的权重极大，从而很大程度上影响最终的期望值。此时，如果采样不充分，没能采到这些权重很大的样本，那么计算得到的样本期望就会跟理论期望差别很大，整套解决方案就垮掉了。

![Var](/img/post-RL-PG-sampling-var.png){:height="75%" width="75%"}

Importance Sampling对于RL的意义在于，可以使用某个policy与环境进行互动得到若干个episode，之后的每次参数更新都只使用这些样本，不再需要每次迭代中都重新采样。

$$\nabla \bar{R}_\theta = E_{(s_t, a_t) \sim p_{\theta}} [A^{\theta}(s_t, a_t) \nabla logp_{\theta}(a_t|s_t)]$$

$$= E_{(s_t, a_t) \sim p_{\theta'}} [\frac{p_{\theta}(a_t, s_t)}{p_{\theta'}(a_t, s_t)} A^{\theta'}(s_t, a_t) \nabla logp_{\theta}(a_t|s_t)]$$

$$= E_{(s_t, a_t) \sim p_{\theta'}} [\frac{p_{\theta}(a_t|s_t)}{p_{\theta'}(a_t|s_t)} A^{\theta'}(a_t, s_t) \nabla logp_{\theta}(a_t|s_t)]$$

其中最后一步假设$p_{\theta}(s) = p_{\theta'}(s)$。最后可以从梯度反推出来目标函数，这个后面PPO会用得上。

$$J^{\theta'}(\theta) = E_{(s_t, a_t) \sim p_{\theta'}} [\frac{p_{\theta}(a_t|s_t)}{p_{\theta'}(a_t|s_t)} A^{\theta'}(s_t, a_t)]$$

## PPO/TRPO

PPO就是利用了上面的思路，在参数变化不大的前提下，不用每次参数更新都去重新采样，可以在参数更新若干次之后再重新采样。TRPO是指一类方法，PPO是其中的一个特例。

![PPO](/img/post-RL-PG-ppo.png){:height="75%" width="75%"}

以上是PPO的目标函数，就上一次的目标函数中加入了$\theta$和$\theta'$所对应分布之间的KL距离。

以下是PPO的整体流程，如上面所述，每次收集的样本可以用来进行多次参数更新，而KL的权重参数也可以根据情况进行调整，使它起的作用不至于过大或过小，即让参数的更新有一定的幅度，又不能变化过大以至于破坏了Importance Sampling的前提假设。

![Proc](/img/post-RL-PG-ppo-proc.png){:height="75%" width="75%"}

但其实这个KL是不容易计算的，因为它涉及不同参数下所有的$p(a\|t)$之间的距离，为了避免计算这个值，就有了PPO2这个改进，如下所示。实际就是将KL距离反应地了目标函数里。当参数变化过大或过小时，它所引起的目标函数的提升会被clip，不会完整地反应地目标函数中，此时的参数对应的目标函数值等价于在region边界的参数所获得的目标函数值，因此就不会有过大的动力（梯度）来更新参数。

![PPO2](/img/post-RL-PG-ppo2.png){:height="75%" width="75%"}



